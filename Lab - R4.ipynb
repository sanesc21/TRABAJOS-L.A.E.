{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2753e4-9dc7-41d7-b080-d1abfbb16b28",
   "metadata": {},
   "source": [
    "# Laboratorio de regresión - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396ea70-b88c-4346-a14f-4aa7ca2e1b00",
   "metadata": {},
   "source": [
    "|                |   |\n",
    ":----------------|---|\n",
    "| **Nombre**     |Santiago Escutia Ríos  |\n",
    "| **Fecha**      | 8/2/2026  |\n",
    "| **Expediente** |  757839 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77def53e-10bf-474e-acdf-728e07bef102",
   "metadata": {},
   "source": [
    "## Modelos penalizados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bb791d-1843-4b4d-bd8b-69e6419511e8",
   "metadata": {},
   "source": [
    "Hasta ahora la función de costo que usamos para decidir qué tan bueno es nuestro modelo al momento de ajustar es:\n",
    "\n",
    "$$ \\text{RSS} = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y_i})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b5e6b-abe9-4b75-b045-e4444de4fc35",
   "metadata": {},
   "source": [
    "Dado que los errores obtenidos son una combinación de sesgo y varianza, puede ser que se sesgue un parámetro para minimizar el error. Esto significa que el modelo puede decidir que la salida no sea una combinación de los factores, sino una fuerte predilección sobre uno de los factores solamente. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84901f9e-5551-455a-a70c-c7e39d9e55ae",
   "metadata": {},
   "source": [
    "E.g. se quiere ajustar un modelo\n",
    "\n",
    "$$ \\hat{z} = \\hat{\\beta_0} + \\hat{\\beta_1} x + \\hat{\\beta_2} y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f473fc-6364-4b15-9bd4-f21a94cae151",
   "metadata": {},
   "source": [
    "Se ajusta el modelo y se decide que la mejor decisión es $\\hat{\\beta_1} = 10000$ y $\\hat{\\beta_2}=50$. Considera limitaciones de problemas reales:\n",
    "- Quizás los parámetros son ajustes de maquinaria que se deben realizar para conseguir el mejor producto posible, y que $10000$ sea imposible de asignar.\n",
    "- Quizás los datos actuales están sesgados y sólo hacen parecer que uno de los factores importa más que el otro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff32fbaa-7965-42c1-9b73-3640414b77f2",
   "metadata": {},
   "source": [
    "Una de las formas en las que se puede mitigar este problema es penalizando a los parámetros del modelo, cambiando la función de costo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc78736-9d8e-4e8b-94f3-6647bdaeb0d1",
   "metadata": {},
   "source": [
    "$$ \\text{RSS}_{L2} = \\sum_{i=1}^n e_i^2  + \\lambda \\sum_{j=1}^p \\hat{\\beta_j}^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d942bf5-fb39-44a0-b612-4ec05ab99b71",
   "metadata": {},
   "source": [
    "El *L2* significa que se está agregando una penalización de segundo orden. Lo que hace esta penalización es que los factores ahora sólo tendrán permitido crecer si hay una reducción al menos proporcional en el error (sacrificamos sesgo, pero reducimos la varianza)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c0cafb-c152-48e4-a345-bb5348eb16c7",
   "metadata": {},
   "source": [
    "Asimismo, existe la penalización *L1*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9f2d93-d151-47ed-834f-4a7e91e94286",
   "metadata": {},
   "source": [
    "$$ \\text{RSS}_{L1} = \\sum_{i=1}^n e_i^2  + \\lambda \\sum_{j=1}^p |\\hat{\\beta_j}| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea95f232-25ab-4b4c-99b3-075a18878d95",
   "metadata": {},
   "source": [
    "A las penalizaciones *L2* y *L1* se les conoce también como Ridge y Lasso, respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41dfafb-fd1f-475a-a718-dec1e3773326",
   "metadata": {},
   "source": [
    "Para realizar una regresión con penalización de Ridge o de Lasso usamos el objeto `Ridge(alpha=?)` o `Lasso(alpha=?)` en lugar de `LinearRegression()` de `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce36cb41-005e-4cb0-b8ab-92e6ae6c4c19",
   "metadata": {},
   "source": [
    "Utiliza el dataset de publicidad (Advertising.csv) y realiza 3 regresiones múltiples:\n",
    "\n",
    "$$ \\text{sales} = \\beta_0 + \\beta_1 (\\text{TV}) + \\beta_2 (\\text{radio}) + \\beta_3 (\\text{newspaper}) + \\epsilon $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92e4008-ada1-4e82-9756-23fd357821d2",
   "metadata": {},
   "source": [
    "1. Sin penalización\n",
    "2. Con penalización L2\n",
    "3. Con penalización L1\n",
    "\n",
    "¿Qué puedes observar al ajustar los valores de `alpha`? \n",
    "\n",
    "Compara los resultados de los coeficientes utilizando valores diferentes de $\\alpha$ y los $R^2$ resultantes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd823476",
   "metadata": {},
   "source": [
    "1. Sin penalización (Regresión normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2912cd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes (Sin penalización): [ 0.04576465  0.18853002 -0.00103749]\n",
      "R^2 (Sin penalización): 0.8972\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df = pd.read_csv('Advertising.csv', index_col=0)\n",
    "X = df[['TV', 'radio', 'newspaper']]\n",
    "y = df['sales']\n",
    "\n",
    "modelo_lr = LinearRegression()\n",
    "modelo_lr.fit(X, y)\n",
    "\n",
    "print(f\"Coeficientes (Sin penalización): {modelo_lr.coef_}\")\n",
    "print(f\"R^2 (Sin penalización): {modelo_lr.score(X, y):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4afcaf3",
   "metadata": {},
   "source": [
    "2. Con penalización L2 (Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac44f8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes (Ridge L2): [ 0.04576446  0.18803935 -0.00091803]\n",
      "R^2 (Ridge L2): 0.8972\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "modelo_ridge = Ridge(alpha=100)\n",
    "modelo_ridge.fit(X, y)\n",
    "\n",
    "print(f\"Coeficientes (Ridge L2): {modelo_ridge.coef_}\")\n",
    "print(f\"R^2 (Ridge L2): {modelo_ridge.score(X, y):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5827b9",
   "metadata": {},
   "source": [
    "Ahora con alphas más altos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d58499f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes (Ridge L2): [0.02818664 0.00804639 0.0040067 ]\n",
      "R^2 (Ridge L2): 0.5414\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "modelo_ridge = Ridge(alpha=1000000)\n",
    "modelo_ridge.fit(X, y)\n",
    "\n",
    "print(f\"Coeficientes (Ridge L2): {modelo_ridge.coef_}\")\n",
    "print(f\"R^2 (Ridge L2): {modelo_ridge.score(X, y):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef188c9",
   "metadata": {},
   "source": [
    "Probando valores alpha diferentes se ve que en valores de 1 a 1,000 no cambia en casi nada, lo que indica que son valores buenos, pero al introducir valores mas grande como 10,000-100,000 el R2 empieza a empeorar considerablemente y los coeficientes se empiezan a acercar mucho a cero (Valores así de altos no hay porque usarse)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0425fa",
   "metadata": {},
   "source": [
    "3. Con penalización L1 (Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53c7d6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes (Lasso L1): [0.04566142 0.1834644  0.        ]\n",
      "R^2 (Lasso L1): 0.8970\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "modelo_lasso = Lasso(alpha=1)\n",
    "modelo_lasso.fit(X, y)\n",
    "\n",
    "print(f\"Coeficientes (Lasso L1): {modelo_lasso.coef_}\")\n",
    "print(f\"R^2 (Lasso L1): {modelo_lasso.score(X, y):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea87f9f",
   "metadata": {},
   "source": [
    "Ahora probando con alphas más altos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b3d4152a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes (Lasso L1): [0.03390169 0.         0.        ]\n",
      "R^2 (Lasso L1): 0.5615\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "modelo_lasso = Lasso(alpha=100)\n",
    "modelo_lasso.fit(X, y)\n",
    "\n",
    "print(f\"Coeficientes (Lasso L1): {modelo_lasso.coef_}\")\n",
    "print(f\"R^2 (Lasso L1): {modelo_lasso.score(X, y):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7771897a",
   "metadata": {},
   "source": [
    "Conforme se aumenta el valor de alpha en Lasso el R2 empieza a empeorar muy rápido (incluso más rápido que en Ridge para valores no tan altos), además de que entre más alto el alpha los coefieicntes eventualmente llegan a cero, algo que que es característico de Lasso. Curiosamente se ve que con alpha=1 ya directamente newspaper da 0 lo que confirma que no es un medio significante para las ventas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
